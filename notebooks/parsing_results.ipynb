{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "IPython.OutputArea.prototype._should_scroll = function(lines) {\n",
       "    return false;\n",
       "}\n"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%javascript\n",
    "IPython.OutputArea.prototype._should_scroll = function(lines) {\n",
    "    return false;\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import gzip\n",
    "import shutil\n",
    "import json\n",
    "import os\n",
    "\n",
    "from pyspark import SparkContext, SparkConf\n",
    "import re\n",
    "\n",
    "from ineqpy import atkinson"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading different statistics from per taxi metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adding `run_id` to the dictionary of results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def add_run_id(fname, d, mode=\"per_taxi\"):\n",
    "    \"\"\"\n",
    "    Given the long filename from Spark, and a dict, this function\n",
    "    cuts out the run_id from the filename, and adds it with a key\n",
    "    `run_id` to the dictionary d.\n",
    "    \"\"\"\n",
    "    if mode==\"per_taxi\":\n",
    "        d[\"run_id\"] = re.sub('^run_','',re.sub(\"_per_taxi_metrics.json.gz$\",'',fname.split('/')[-1]))\n",
    "    elif mode==\"aggregates\":\n",
    "        d[\"run_id\"] = re.sub('^run_','',re.sub(\"_aggregates.csv.gz$\",'',fname.split('/')[-1]))\n",
    "    return d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we could calculate everything we need from the incomes, online ratios etc. at once."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def calc_atkinson(t):\n",
    "    \"\"\"\n",
    "    Given a dictionary with the results, it calculates the Atkinson index\n",
    "    of the incomes.\n",
    "    \"\"\"\n",
    "    if \"trip_avg_price\" in t:\n",
    "        return {\"run_id\" : t[\"run_id\"], \"atkinson\" : atkinson(np.array(list(map(float,t[\"trip_avg_price\"]))))}\n",
    "    elif \"trip_income\" in t:\n",
    "        return {\"run_id\" : t[\"run_id\"], \"atkinson\" : atkinson(np.array(list(map(float,t[\"trip_income\"]))))}\n",
    "    else:\n",
    "        return {\"run_id\" : t[\"run_id\"], \"atkinson\" : None } "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Spark settings\n",
    "conf = SparkConf().set(\"spark.executor.memory\", \"20g\")\n",
    "\n",
    "# removing old output directory\n",
    "shutil.rmtree(\"atkinson\")\n",
    "\n",
    "# initializing a SparkContext object\n",
    "sc = SparkContext(conf=conf)\n",
    "\n",
    "# defining a Spark RDD and saving the results into a textfile\n",
    "sc.wholeTextFiles('results/run_2019*_per_taxi_metrics.json.gz')\\\n",
    ".map(lambda t: add_run_id(t[0],json.loads(t[1].split('\\n')[-2])))\\\n",
    ".map(lambda t: calc_atkinson(t))\\\n",
    ".saveAsTextFile(\"atkinson\")\n",
    "\n",
    "# stopping the SparkContext\n",
    "sc.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# collecting results from Spark\n",
    "atkinson_df  = pd.DataFrame.from_dict(\n",
    "    [\n",
    "        json.loads(l.strip('\\n').replace('\\'','\\\"')) \\\n",
    "         for f in os.listdir('atkinson/') \\\n",
    "         if 'part-'==f[0:5] \\\n",
    "         for l in open('atkinson/'+f).readlines()\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading the configuration data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "configs = []\n",
    "with gzip.open('configs/2019_all.conf.gz') as f:\n",
    "    for line in f.readlines():\n",
    "        configs.append(json.loads(line))\n",
    "\n",
    "configs_df = pd.DataFrame.from_dict(configs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading the aggregate csv files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# removing old output directory\n",
    "shutil.rmtree(\"temp_data/aggregates\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# initializing a SparkContext object\n",
    "sc = SparkContext(conf=conf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# defining a Spark RDD and saving the results into a textfile\n",
    "sc.wholeTextFiles('results/run_2019*.csv.gz')\\\n",
    ".map(lambda t: add_run_id(t[0],pd.DataFrame.from_csv(t[0]).iloc[-1,].to_dict(),mode=\"aggregates\"))\\\n",
    ".saveAsTextFile(\"temp_data/aggregates\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# stopping the SparkContext\n",
    "sc.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Somehow I could not find a solution in Python, so here is a first small preparation in bash."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%bash \n",
    "\n",
    "cat aggregates/part* | sed 's/\\x27/\\\"/g' | sed 's/nan/null/g' | grep -v '{79.0:' | jq -c '.' > aggregates.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# collecting results from Spark\n",
    "aggregates_df  = pd.DataFrame.from_dict(\n",
    "    [\n",
    "        json.loads(l.strip('\\n')) for l in open('aggregates.json').readlines()\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merging and cleaning the joined results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# setting run_ids as index for all three dataframes\n",
    "configs_df['run_id'] = configs_df['run_id'].map(lambda s: re.sub('\\.conf','',s))\n",
    "configs_df.set_index('run_id',inplace=True)\n",
    "atkinson_df.set_index('run_id',inplace=True)\n",
    "aggregates_df.set_index('run_id',inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# joining the different results\n",
    "merged = configs_df.join(atkinson_df).join(aggregates_df,rsuffix=\"_2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# dropping one row that belongs to the base config\n",
    "merged.drop('2019_02_14_base',inplace=True)\n",
    "# adding a geom column from the run_ids\n",
    "merged[\"geom\"]=merged.index.map(lambda s: re.sub(r'^.+geom_([0-9]).+$',r'\\1',s))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def select_not_nan(x,y):\n",
    "    \"\"\"\n",
    "    Out of two elements, returns the one that is not a pandas NaN.\n",
    "    If both are NaNs, returns a NaN.\n",
    "    \"\"\"\n",
    "    if pd.isnull(x):\n",
    "        return y\n",
    "    else:\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# merging names from old and new code to one single column\n",
    "for prefix in ['avg', 'std']:\n",
    "    merged[prefix+'_trip_avg_price'] = merged[prefix+'_trip_avg_price'].combine(merged[prefix+'_trip_income'],select_not_nan)\n",
    "    merged.drop(prefix+'_trip_income',axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# deleting unnecessary columns\n",
    "merged.drop([\n",
    "    \"show_plot\", \n",
    "    \"show_map_labels\", \n",
    "    \"show_pending\", \n",
    "    \"avg_timestamp\", \n",
    "    \"std_timestamp\",\n",
    "    \"0\",\n",
    "    \"avg_request_lengths_2\"\n",
    "],axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# saving results\n",
    "merged.to_csv('notebooks/results.csv.gz',compression='gzip')"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "py36",
   "language": "python",
   "name": "py36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  },
  "toc": {
   "toc_cell": false,
   "toc_number_sections": true,
   "toc_threshold": 6,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
